{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"}],"dockerImageVersionId":30674,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I add the meaning almost for each cell for starter can easily understand whats the usage. rename some variables name to easy read","metadata":{"execution":{"iopub.status.busy":"2024-04-19T00:42:20.138879Z","iopub.execute_input":"2024-04-19T00:42:20.139760Z","iopub.status.idle":"2024-04-19T00:42:20.146043Z","shell.execute_reply.started":"2024-04-19T00:42:20.139695Z","shell.execute_reply":"2024-04-19T00:42:20.144886Z"}}},{"cell_type":"markdown","source":"# Import some libs","metadata":{}},{"cell_type":"code","source":"import sys\nfrom pathlib import Path\nimport subprocess\nimport os\nimport gc\nfrom glob import glob\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom datetime import datetime\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-20T06:48:04.600521Z","iopub.execute_input":"2024-04-20T06:48:04.601234Z","iopub.status.idle":"2024-04-20T06:48:06.947192Z","shell.execute_reply.started":"2024-04-20T06:48:04.601201Z","shell.execute_reply":"2024-04-20T06:48:06.946399Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.impute import KNNImputer","metadata":{"execution":{"iopub.status.busy":"2024-04-20T06:48:10.367946Z","iopub.execute_input":"2024-04-20T06:48:10.368772Z","iopub.status.idle":"2024-04-20T06:48:14.008066Z","shell.execute_reply.started":"2024-04-20T06:48:10.368739Z","shell.execute_reply":"2024-04-20T06:48:14.007255Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# define some functions for load dataset","metadata":{}},{"cell_type":"code","source":"class Pipeline:\n\n    def set_table_dtypes(df):\n        for col in df.columns:\n            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Int64))\n            elif col in [\"date_decision\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n            elif col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float64))\n            elif col[-1] in (\"M\",):\n                df = df.with_columns(pl.col(col).cast(pl.String))\n            elif col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n        return df\n\n    def handle_dates(df):\n        for col in df.columns:\n            if col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  #!!?\n                df = df.with_columns(pl.col(col).dt.total_days()) # t - t-1\n        df = df.drop(\"date_decision\", \"MONTH\")\n        return df\n\n    def filter_cols(df):\n        # drop the col if missing value > 0.7\n        for col in df.columns:\n            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n                isnull = df[col].is_null().mean()\n                if isnull > 0.7:\n                    df = df.drop(col)\n        # Remove Irrelevant or Overly Complex Categorical Columns:\n        for col in df.columns:\n            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n                freq = df[col].n_unique()\n                if (freq == 1) | (freq > 200):\n                    df = df.drop(col)\n        \n        return df","metadata":{"execution":{"iopub.status.busy":"2024-04-20T06:48:17.812774Z","iopub.execute_input":"2024-04-20T06:48:17.813150Z","iopub.status.idle":"2024-04-20T06:48:17.825590Z","shell.execute_reply.started":"2024-04-20T06:48:17.813119Z","shell.execute_reply":"2024-04-20T06:48:17.824502Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class Aggregator:\n    #Please add or subtract features yourself, be aware that too many features will take up too much space.\n    def num_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        \n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n        return expr_max +expr_last+expr_mean\n    \n    def date_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"D\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n        return  expr_max +expr_last+expr_mean\n    \n    def str_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        #expr_count = [pl.count(col).alias(f\"count_{col}\") for col in cols]\n        return  expr_max +expr_last#+expr_count\n    \n    def other_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        return  expr_max +expr_last\n    \n    def count_expr(df):\n        cols = [col for col in df.columns if \"num_group\" in col]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols] \n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        return  expr_max +expr_last\n    \n    def get_exprs(df):\n        exprs = Aggregator.num_expr(df) + \\\n                Aggregator.date_expr(df) + \\\n                Aggregator.str_expr(df) + \\\n                Aggregator.other_expr(df) + \\\n                Aggregator.count_expr(df)\n\n        return exprs","metadata":{"execution":{"iopub.status.busy":"2024-04-20T06:48:21.213300Z","iopub.execute_input":"2024-04-20T06:48:21.213950Z","iopub.status.idle":"2024-04-20T06:48:21.228239Z","shell.execute_reply.started":"2024-04-20T06:48:21.213919Z","shell.execute_reply":"2024-04-20T06:48:21.227325Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## how to read file","metadata":{}},{"cell_type":"code","source":"def read_file(path, depth = None):\n    df = pl.read_parquet(path)\n    df = df.pipe(Pipeline.set_table_dtypes)\n    if depth in [1,2]:\n        df = df.group_by('case_id').agg(Aggregator.get_exprs(df))\n    return df\n\ndef read_files(regex_path,depth = None):\n    chunks = []\n    \n    for path in glob(str(regex_path)):\n        df = pl.read_parquet(path)\n        df = df.pipe(Pipeline.set_table_dtypes)\n        if depth in [1, 2]:\n            df = df.group_by('case_id').agg(Aggregator.get_exprs(df))\n        chunks.append(df)\n        \n    df = pl.concat(chunks,how = 'vertical_relaxed')\n    df = df.unique(subset = ['case_id'])\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-20T06:48:27.419374Z","iopub.execute_input":"2024-04-20T06:48:27.420248Z","iopub.status.idle":"2024-04-20T06:48:27.427605Z","shell.execute_reply.started":"2024-04-20T06:48:27.420217Z","shell.execute_reply":"2024-04-20T06:48:27.426506Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## feature engineering: add new col in df_base & left join different depth file togather","metadata":{}},{"cell_type":"code","source":"def feature_eng(df_base, depth_0, depth_1, depth_2):\n    df_base = (\n        df_base\n        .with_columns(\n            month_decision = pl.col(\"date_decision\").dt.month(),\n            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n        )\n    )\n    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n    df_base = df_base.pipe(Pipeline.handle_dates)\n    return df_base","metadata":{"execution":{"iopub.status.busy":"2024-04-20T06:48:29.794275Z","iopub.execute_input":"2024-04-20T06:48:29.794946Z","iopub.status.idle":"2024-04-20T06:48:29.801010Z","shell.execute_reply.started":"2024-04-20T06:48:29.794916Z","shell.execute_reply":"2024-04-20T06:48:29.800027Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def to_pandas(dataframe, categorical_columns=None):\n    # Convert the dataframe to a pandas dataframe\n    pandas_df = dataframe.to_pandas()\n    \n    # If categorical_columns is not provided, identify all object dtype columns as categorical\n    if categorical_columns is None:\n        categorical_columns = list(pandas_df.select_dtypes(\"object\").columns)\n    \n    # Convert identified columns to categorical dtype\n    pandas_df[categorical_columns] = pandas_df[categorical_columns].astype(\"category\")\n    \n    # Return the pandas dataframe and the list of categorical columns\n    return pandas_df, categorical_columns","metadata":{"execution":{"iopub.status.busy":"2024-04-20T06:48:33.436550Z","iopub.execute_input":"2024-04-20T06:48:33.437335Z","iopub.status.idle":"2024-04-20T06:48:33.442232Z","shell.execute_reply.started":"2024-04-20T06:48:33.437307Z","shell.execute_reply":"2024-04-20T06:48:33.441253Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## decrease the memory by using lightest number type","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(dataframe):\n    # Calculate the initial memory usage of the DataFrame\n    initial_memory_usage = dataframe.memory_usage().sum() / 1024**2\n    print(f'Initial memory usage: {initial_memory_usage:.2f} MB')\n    \n    # Iterate over each column in the DataFrame\n    for column_name in dataframe.columns:\n        column_dtype = dataframe[column_name].dtype\n\n        # Skip the optimization for categorical columns\n        if str(column_dtype) == \"category\":\n            continue\n\n        # Skip optimization for non-numeric columns\n        if column_dtype != 'object':\n            min_value = dataframe[column_name].min()\n            max_value = dataframe[column_name].max()\n\n            # Downcast integer columns to the smallest integer dtype possible\n            if 'int' in str(column_dtype):\n                if min_value >= np.iinfo(np.int8).min and max_value <= np.iinfo(np.int8).max:\n                    dataframe[column_name] = dataframe[column_name].astype(np.int8)\n                elif min_value >= np.iinfo(np.int16).min and max_value <= np.iinfo(np.int16).max:\n                    dataframe[column_name] = dataframe[column_name].astype(np.int16)\n                elif min_value >= np.iinfo(np.int32).min and max_value <= np.iinfo(np.int32).max:\n                    dataframe[column_name] = dataframe[column_name].astype(np.int32)\n                elif min_value >= np.iinfo(np.int64).min and max_value <= np.iinfo(np.int64).max:\n                    dataframe[column_name] = dataframe[column_name].astype(np.int64)  \n\n            # Downcast float columns to the smallest float dtype possible\n            else:\n                if min_value >= np.finfo(np.float16).min and max_value <= np.finfo(np.float16).max:\n                    dataframe[column_name] = dataframe[column_name].astype(np.float16)\n                elif min_value >= np.finfo(np.float32).min and max_value <= np.finfo(np.float32).max:\n                    dataframe[column_name] = dataframe[column_name].astype(np.float32)\n                else:\n                    dataframe[column_name] = dataframe[column_name].astype(np.float64)\n\n        else:\n            continue\n\n    # Calculate the final memory usage after optimization\n    final_memory_usage = dataframe.memory_usage().sum() / 1024**2\n    print(f'Final memory usage: {final_memory_usage:.2f} MB (reduced by {(initial_memory_usage - final_memory_usage) / initial_memory_usage * 100:.1f}%)')\n    \n    # Return the optimized DataFrame\n    return dataframe","metadata":{"execution":{"iopub.status.busy":"2024-04-20T06:48:36.245666Z","iopub.execute_input":"2024-04-20T06:48:36.246011Z","iopub.status.idle":"2024-04-20T06:48:36.259715Z","shell.execute_reply.started":"2024-04-20T06:48:36.245982Z","shell.execute_reply":"2024-04-20T06:48:36.258737Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# load the train files","metadata":{}},{"cell_type":"markdown","source":"## define the path","metadata":{}},{"cell_type":"code","source":"ROOT            = Path('/kaggle/input/home-credit-credit-risk-model-stability')\nTRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\nTEST_DIR        = ROOT / \"parquet_files\" / \"test\"","metadata":{"execution":{"iopub.status.busy":"2024-04-20T06:48:39.389176Z","iopub.execute_input":"2024-04-20T06:48:39.389579Z","iopub.status.idle":"2024-04-20T06:48:39.394382Z","shell.execute_reply.started":"2024-04-20T06:48:39.389530Z","shell.execute_reply":"2024-04-20T06:48:39.393266Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## load in data_store","metadata":{}},{"cell_type":"code","source":"data_store = {\n    'df_base': read_file(TRAIN_DIR / 'train_base.parquet'),\n    'depth_0':[\n        read_file(TRAIN_DIR / 'train_static_cb_0.parquet'),\n        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n    ],\n     'depth_1': [\n        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),  \n     ],\n     'depth_2':[\n        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n        read_file(TRAIN_DIR / \"train_applprev_2.parquet\", 2),\n        read_file(TRAIN_DIR / \"train_person_2.parquet\", 2)\n         \n     ]\n}","metadata":{"execution":{"iopub.status.busy":"2024-04-20T06:48:42.395137Z","iopub.execute_input":"2024-04-20T06:48:42.395526Z","iopub.status.idle":"2024-04-20T06:51:00.260604Z","shell.execute_reply.started":"2024-04-20T06:48:42.395497Z","shell.execute_reply":"2024-04-20T06:51:00.259709Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## exec feature engi => get the one overall df","metadata":{}},{"cell_type":"code","source":"df_train = feature_eng(**data_store)#**传参，不需要提前知道字典中有哪些键，可以更加灵活地传递参数\nprint(\"train data shape:\\t\", df_train.shape)#获得DataFrame维度（行与列）","metadata":{"execution":{"iopub.status.busy":"2024-04-20T06:51:10.605439Z","iopub.execute_input":"2024-04-20T06:51:10.606041Z","iopub.status.idle":"2024-04-20T06:51:27.407864Z","shell.execute_reply.started":"2024-04-20T06:51:10.606001Z","shell.execute_reply":"2024-04-20T06:51:27.406681Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"train data shape:\t (1526659, 861)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Deleted large objects and free up memory sooner","metadata":{}},{"cell_type":"code","source":"del data_store\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T07:17:10.427275Z","iopub.execute_input":"2024-04-20T07:17:10.427979Z","iopub.status.idle":"2024-04-20T07:17:10.457673Z","shell.execute_reply.started":"2024-04-20T07:17:10.427945Z","shell.execute_reply":"2024-04-20T07:17:10.456392Z"},"trusted":true},"execution_count":18,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m data_store\n\u001b[1;32m      2\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n","\u001b[0;31mNameError\u001b[0m: name 'data_store' is not defined"],"ename":"NameError","evalue":"name 'data_store' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"## Do the filter for all cols, convert to pandas df & reduce the mem usage of df","metadata":{}},{"cell_type":"code","source":"df_train = df_train.pipe(Pipeline.filter_cols)\ndf_train, cat_cols = to_pandas(df_train)\ndf_train = reduce_mem_usage(df_train)\nprint(\"train data shape:\\t\", df_train.shape)#优化后","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get the group numerical Columns by Missing Value Counts ","metadata":{}},{"cell_type":"code","source":"nums=df_train.select_dtypes(exclude='category').columns#排除分类变量，选择数值型\nfrom itertools import combinations, permutations\n#df_train=df_train[nums]\nnans_df = df_train[nums].isna()#判断缺失值\nnans_groups={}\nfor col in nums:\n    cur_group = nans_df[col].sum()#缺失值相加\n    try:\n        nans_groups[cur_group].append(col)\n    except:\n        nans_groups[cur_group]=[col]\ndel nans_df; x=gc.collect()\n\n# as the result, get the nans_groups\n# {\n#     0: [\"column1\", \"column2\"],  # No missing values in these columns\n#     5: [\"column3\"],             # 5 missing values in column3\n#     10: [\"column4\", \"column5\"], # 10 missing values in each of these columns\n# }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define two helper func to handle nans_df","metadata":{}},{"cell_type":"code","source":"# selecting columns based on the maximum number of unique values.\ndef reduce_group(groups):\n    selected_columns = []  # This will store the column names selected from each group\n    for group in groups:\n        max_uniques = 0  # Initialize the maximum number of unique values found\n        selected_column = group[0]  # Default to the first column in the group as a fallback\n        \n        for column in group:\n            unique_count = df_train[column].nunique()  # Number of unique values in the column\n            if unique_count > max_uniques:\n                max_uniques = unique_count\n                selected_column = column\n        \n        selected_columns.append(selected_column)\n    \n    print('Selected columns based on max uniques:', selected_columns)\n    return selected_columns\n\n\n\ndef group_columns_by_correlation(dataframe, correlation_threshold=0.8):\n    # Calculate the correlation matrix for the dataframe\n    correlation_matrix = dataframe.corr()\n\n    # Initialize a list to hold all groups of correlated columns\n    correlated_groups = []\n\n    # List of columns yet to be grouped\n    remaining_columns = list(dataframe.columns)\n\n    while remaining_columns:\n        # Pop the first column name off the remaining list to create a new correlation group\n        current_column = remaining_columns.pop(0)\n        current_group = [current_column]\n        correlated_columns = [current_column]  # This will hold all columns that correlate with `current_column`\n\n        # Iterate over the remaining columns and check if the correlation with the current column is above the threshold\n        for other_column in remaining_columns:\n            if correlation_matrix.loc[current_column, other_column] >= correlation_threshold:\n                current_group.append(other_column)\n                correlated_columns.append(other_column)\n\n        # Append the group to the list of groups\n        correlated_groups.append(current_group)\n\n        # Update remaining_columns by removing the correlated columns\n        remaining_columns = [col for col in remaining_columns if col not in correlated_columns]\n\n    return correlated_groups\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = feature_eng(**data_store)#**传参，不需要提前知道字典中有哪些键，可以更加灵活地传递参数\nprint(\"train data shape:\\t\", df_train.shape)#获得DataFrame维度（行与列）\ndel data_store\ngc.collect()#内存回收\ndf_train = df_train.pipe(Pipeline.filter_cols)\ndf_train, cat_cols = to_pandas(df_train)\ndf_train = reduce_mem_usage(df_train)\nprint(\"train data shape:\\t\", df_train.shape)#优化后\n\nnums=df_train.select_dtypes(exclude='category').columns#排除分类变量，选择数值型\nfrom itertools import combinations, permutations\n#df_train=df_train[nums]\nnans_df = df_train[nums].isna()#判断缺失值\nnans_groups={}\nfor col in nums:\n    cur_group = nans_df[col].sum()#缺失值相加\n    try:\n        nans_groups[cur_group].append(col)\n    except:\n        nans_groups[cur_group]=[col]\ndel nans_df; x=gc.collect()\n\n# selecting columns based on the maximum number of unique values.\ndef reduce_group(groups):\n    selected_columns = []  # This will store the column names selected from each group\n    for group in groups:\n        max_uniques = 0  # Initialize the maximum number of unique values found\n        selected_column = group[0]  # Default to the first column in the group as a fallback\n        \n        for column in group:\n            unique_count = df_train[column].nunique()  # Number of unique values in the column\n            if unique_count > max_uniques:\n                max_uniques = unique_count\n                selected_column = column\n        \n        selected_columns.append(selected_column)\n    \n    print('Selected columns based on max uniques:', selected_columns)\n    return selected_columns\n\n\n\ndef group_columns_by_correlation(dataframe, correlation_threshold=0.8):\n    # Calculate the correlation matrix for the dataframe\n    correlation_matrix = dataframe.corr()\n\n    # Initialize a list to hold all groups of correlated columns\n    correlated_groups = []\n\n    # List of columns yet to be grouped\n    remaining_columns = list(dataframe.columns)\n\n    while remaining_columns:\n        # Pop the first column name off the remaining list to create a new correlation group\n        current_column = remaining_columns.pop(0)\n        current_group = [current_column]\n        correlated_columns = [current_column]  # This will hold all columns that correlate with `current_column`\n\n        # Iterate over the remaining columns and check if the correlation with the current column is above the threshold\n        for other_column in remaining_columns:\n            if correlation_matrix.loc[current_column, other_column] >= correlation_threshold:\n                current_group.append(other_column)\n                correlated_columns.append(other_column)\n\n        # Append the group to the list of groups\n        correlated_groups.append(current_group)\n\n        # Update remaining_columns by removing the correlated columns\n        remaining_columns = [col for col in remaining_columns if col not in correlated_columns]\n\n    return correlated_groups\n\nuses=[]\nfor k,v in nans_groups.items():\n    if len(v)>1:\n            Vs = nans_groups[k]\n            #cross_features=list(combinations(Vs, 2))\n            #make_corr(Vs)\n            grps= group_columns_by_correlation(df_train[Vs], threshold=0.8)\n            use=reduce_group(grps)\n            uses=uses+use\n            #make_corr(use)\n    else:\n        uses=uses+v\n    print('####### NAN count =',k)\nprint(uses)\nprint(len(uses))\nuses=uses+list(df_train.select_dtypes(include='category').columns)\nprint(len(uses))\ndf_train=df_train[uses]","metadata":{"execution":{"iopub.status.busy":"2024-04-19T00:42:16.928436Z","iopub.status.idle":"2024-04-19T00:42:16.928826Z","shell.execute_reply.started":"2024-04-19T00:42:16.928648Z","shell.execute_reply":"2024-04-19T00:42:16.928664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = pd.read_csv(\"/kaggle/input/home-credit-credit-risk-model-stability/sample_submission.csv\")\ndevice='gpu'\n#n_samples=200000\nn_est=6000\nDRY_RUN = True if sample.shape[0] == 10 else False   \nif DRY_RUN:\n    device='cpu'\n    df_train = df_train.iloc[:50000]\n    #n_samples=10000\n    n_est = 600\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T00:42:16.930122Z","iopub.status.idle":"2024-04-19T00:42:16.930534Z","shell.execute_reply.started":"2024-04-19T00:42:16.930346Z","shell.execute_reply":"2024-04-19T00:42:16.930363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# load the test file into data_store","metadata":{}},{"cell_type":"code","source":"data_store = {\n    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n    \"depth_0\": [\n        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n    ],\n    \"depth_1\": [\n        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n    ],\n    \"depth_2\": [\n        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n        read_file(TEST_DIR / \"test_applprev_2.parquet\", 2),\n        read_file(TEST_DIR / \"test_person_2.parquet\", 2)\n    ]\n}","metadata":{"execution":{"iopub.status.busy":"2024-04-19T00:42:16.931854Z","iopub.status.idle":"2024-04-19T00:42:16.932180Z","shell.execute_reply.started":"2024-04-19T00:42:16.932016Z","shell.execute_reply":"2024-04-19T00:42:16.932030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = feature_eng(**data_store)\nprint(\"test data shape:\\t\", df_test.shape)\ndel data_store\ngc.collect()\ndf_test = df_test.select([col for col in df_train.columns if col != \"target\"])\nprint(\"train data shape:\\t\", df_train.shape)\nprint(\"test data shape:\\t\", df_test.shape)\n\ndf_test, cat_cols = to_pandas(df_test, cat_cols)\ndf_test = reduce_mem_usage(df_test)\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-04-19T00:42:16.936066Z","iopub.status.idle":"2024-04-19T00:42:16.936449Z","shell.execute_reply.started":"2024-04-19T00:42:16.936271Z","shell.execute_reply":"2024-04-19T00:42:16.936287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n#遍历键k-值v\nfor k, v in nans_groups.items():\n    if len(v) > 1:\n        # 检查列是否存在于DataFrame中\n        if all(col in df_train.columns for col in v):\n            # 计算相关性矩阵\n            correlation_matrix = df_train[v].corr()\n            \n            # 绘制热力图\n            plt.figure(figsize=(10, 8))\n            sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n            plt.title(f'Correlation Matrix (NAN count = {k})')\n            plt.show()\n        else:\n            print(f\"Columns {v} not found in DataFrame.\")\n\n    print('####### NAN count =', k)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T00:42:16.937587Z","iopub.status.idle":"2024-04-19T00:42:16.937936Z","shell.execute_reply.started":"2024-04-19T00:42:16.937762Z","shell.execute_reply":"2024-04-19T00:42:16.937777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df_train ['target']\nweeks = df_train['WEEK_NUM']\ndf_train = df_train.drop(columns = ['target','case_id','WEEK_NUM'])\ncv = StratifiedGroupKFold(n_splits=5, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T00:42:16.940385Z","iopub.status.idle":"2024-04-19T00:42:16.941128Z","shell.execute_reply.started":"2024-04-19T00:42:16.940864Z","shell.execute_reply":"2024-04-19T00:42:16.940886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[cat_cols] = df_train[cat_cols].astype(str)\ndf_test[cat_cols] = df_test[cat_cols].astype(str)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T00:42:16.943055Z","iopub.status.idle":"2024-04-19T00:42:16.943449Z","shell.execute_reply.started":"2024-04-19T00:42:16.943259Z","shell.execute_reply":"2024-04-19T00:42:16.943276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostClassifier, Pool\n\nfitted_models = []\ncv_scores = []\n\n\nfor idx_train, idx_valid in cv.split(df_train, y, groups=weeks):#\n    X_train, y_train = df_train.iloc[idx_train], y.iloc[idx_train]# \n    X_valid, y_valid = df_train.iloc[idx_valid], y.iloc[idx_valid]\n    train_pool = Pool(X_train, y_train,cat_features=cat_cols)\n    val_pool = Pool(X_valid, y_valid,cat_features=cat_cols)\n    clf = CatBoostClassifier(\n    eval_metric='AUC',\n    task_type='GPU',\n    learning_rate=0.03,\n    iterations=n_est)\n    clf.fit(train_pool, eval_set=val_pool,verbose=300)\n    fitted_models.append(clf)\n    y_pred_valid = clf.predict_proba(X_valid)[:,1]\n    auc_score = roc_auc_score(y_valid, y_pred_valid)\n    cv_scores.append(auc_score)\n    \nprint(\"CV AUC scores: \", cv_scores)\nprint(\"Maximum CV AUC score: \", max(cv_scores))","metadata":{"execution":{"iopub.status.busy":"2024-04-19T00:42:16.944956Z","iopub.status.idle":"2024-04-19T00:42:16.945385Z","shell.execute_reply.started":"2024-04-19T00:42:16.945165Z","shell.execute_reply":"2024-04-19T00:42:16.945181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VotingModel(BaseEstimator, RegressorMixin):\n    def __init__(self, estimators):\n        super().__init__()\n        self.estimators = estimators\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def predict(self, X):\n        y_preds = [estimator.predict(X) for estimator in self.estimators]\n        return np.mean(y_preds, axis=0)\n    \n    def predict_proba(self, X):\n        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n        return np.mean(y_preds, axis=0)\n\nmodel = VotingModel(fitted_models)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T00:42:16.946859Z","iopub.status.idle":"2024-04-19T00:42:16.947272Z","shell.execute_reply.started":"2024-04-19T00:42:16.947044Z","shell.execute_reply":"2024-04-19T00:42:16.947060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance =fitted_models[2].get_feature_importance(type='PredictionValuesChange')\n\n# 获取特征名称\nfeature_names = X_train.columns\n\n# 对特征重要性进行排序\nsorted_idx = np.argsort(feature_importance)\n\n# 绘制特征重要性图\nplt.figure(figsize=(20, 60))\nplt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\nplt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\nplt.xlabel('Feature Importance')\nplt.ylabel('Features')\nplt.title('CatBoost Feature Importance')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-19T00:42:16.949173Z","iopub.status.idle":"2024-04-19T00:42:16.949708Z","shell.execute_reply.started":"2024-04-19T00:42:16.949458Z","shell.execute_reply":"2024-04-19T00:42:16.949481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = df_test.drop(columns=[\"WEEK_NUM\"])\ndf_test = df_test.set_index(\"case_id\")\n\n\ny_pred = pd.Series(model.predict_proba(df_test)[:, 1], index=df_test.index)\ndf_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\ndf_subm = df_subm.set_index(\"case_id\")\n\ndf_subm[\"score\"] = y_pred\ndf_subm.to_csv(\"submission.csv\")\ndf_subm","metadata":{"execution":{"iopub.status.busy":"2024-04-19T00:42:16.951875Z","iopub.status.idle":"2024-04-19T00:42:16.952375Z","shell.execute_reply.started":"2024-04-19T00:42:16.952102Z","shell.execute_reply":"2024-04-19T00:42:16.952123Z"},"trusted":true},"execution_count":null,"outputs":[]}]}