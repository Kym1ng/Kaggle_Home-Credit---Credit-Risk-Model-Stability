{"cells":[{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-04-19T00:42:20.139760Z","iopub.status.busy":"2024-04-19T00:42:20.138879Z","iopub.status.idle":"2024-04-19T00:42:20.146043Z","shell.execute_reply":"2024-04-19T00:42:20.144886Z","shell.execute_reply.started":"2024-04-19T00:42:20.139695Z"}},"source":["I add the meaning almost for each cell for starter can easily understand whats the usage. rename some variables name to easy read"]},{"cell_type":"markdown","metadata":{},"source":["# Import some libs"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T06:48:04.601234Z","iopub.status.busy":"2024-04-20T06:48:04.600521Z","iopub.status.idle":"2024-04-20T06:48:06.947192Z","shell.execute_reply":"2024-04-20T06:48:06.946399Z","shell.execute_reply.started":"2024-04-20T06:48:04.601201Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n","Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"]},{"ename":"ModuleNotFoundError","evalue":"No module named 'polars'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpolars\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'polars'"]}],"source":["import sys\n","from pathlib import Path\n","import subprocess\n","import os\n","import gc\n","from glob import glob\n","\n","import numpy as np\n","import pandas as pd\n","import polars as pl\n","from datetime import datetime\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T06:48:10.368772Z","iopub.status.busy":"2024-04-20T06:48:10.367946Z","iopub.status.idle":"2024-04-20T06:48:14.008066Z","shell.execute_reply":"2024-04-20T06:48:14.007255Z","shell.execute_reply.started":"2024-04-20T06:48:10.368739Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\n","from sklearn.base import BaseEstimator, RegressorMixin\n","from sklearn.metrics import roc_auc_score\n","import lightgbm as lgb\n","\n","from imblearn.over_sampling import SMOTE\n","from sklearn.preprocessing import OrdinalEncoder\n","from sklearn.impute import KNNImputer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# define the dataset directory\n","PATH_BASE_TRAIN = f'{ROOT}/csv_files/train/train_base.csv'\n","PATH_BASE_TEST = f'{ROOT}/csv_files/test/test_base.csv'\n","\n","# load the dataset\n","train =  pd.read_csv(PATH_BASE_TRAIN)\n","test =   pd.read_csv(PATH_BASE_TEST)\n","\n","# Display the shape of the datasets\n","print('The shape of the train dataset is:', train.shape)\n","print('The shape of the test dataset is:', test.shape)\n","print('-------------------------------------------------\\n\\n')\n","\n","# Display the first 3 rows of the datasets\n","print('Display the first 3 rows of the train dataset:')\n","display(train.head(3))\n","print('Display the first 3 rows of the test dataset:')\n","display(test.head(3))\n","print('-------------------------------------------------')\n","\n","# Display the data types of the datasets\n","print('Display the data types of the train dataset:')\n","display(train.dtypes)\n","print('-----------------------------------------------')\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# define some functions for load dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T06:48:17.813150Z","iopub.status.busy":"2024-04-20T06:48:17.812774Z","iopub.status.idle":"2024-04-20T06:48:17.825590Z","shell.execute_reply":"2024-04-20T06:48:17.824502Z","shell.execute_reply.started":"2024-04-20T06:48:17.813119Z"},"trusted":true},"outputs":[],"source":["class Pipeline:\n","\n","    def set_table_dtypes(df):\n","        for col in df.columns:\n","            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n","                df = df.with_columns(pl.col(col).cast(pl.Int64))\n","            elif col in [\"date_decision\"]:\n","                df = df.with_columns(pl.col(col).cast(pl.Date))\n","            elif col[-1] in (\"P\", \"A\"):\n","                df = df.with_columns(pl.col(col).cast(pl.Float64))\n","            elif col[-1] in (\"M\",):\n","                df = df.with_columns(pl.col(col).cast(pl.String))\n","            elif col[-1] in (\"D\",):\n","                df = df.with_columns(pl.col(col).cast(pl.Date))\n","        return df\n","\n","    def handle_dates(df):\n","        for col in df.columns:\n","            if col[-1] in (\"D\",):\n","                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  #!!?\n","                df = df.with_columns(pl.col(col).dt.total_days()) # t - t-1\n","        df = df.drop(\"date_decision\", \"MONTH\")\n","        return df\n","\n","    def filter_cols(df):\n","        # drop the col if missing value > 0.7\n","        for col in df.columns:\n","            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n","                isnull = df[col].is_null().mean()\n","                if isnull > 0.7:\n","                    df = df.drop(col)\n","        # Remove Irrelevant or Overly Complex Categorical Columns:\n","        for col in df.columns:\n","            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n","                freq = df[col].n_unique()\n","                if (freq == 1) | (freq > 200):\n","                    df = df.drop(col)\n","        \n","        return df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T06:48:21.213950Z","iopub.status.busy":"2024-04-20T06:48:21.213300Z","iopub.status.idle":"2024-04-20T06:48:21.228239Z","shell.execute_reply":"2024-04-20T06:48:21.227325Z","shell.execute_reply.started":"2024-04-20T06:48:21.213919Z"},"trusted":true},"outputs":[],"source":["class Aggregator:\n","    #Please add or subtract features yourself, be aware that too many features will take up too much space.\n","    def num_expr(df):\n","        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","        \n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n","        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n","        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n","        return expr_max +expr_last+expr_mean\n","    \n","    def date_expr(df):\n","        cols = [col for col in df.columns if col[-1] in (\"D\")]\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n","        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n","        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n","        return  expr_max +expr_last+expr_mean\n","    \n","    def str_expr(df):\n","        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n","        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n","        #expr_count = [pl.count(col).alias(f\"count_{col}\") for col in cols]\n","        return  expr_max +expr_last#+expr_count\n","    \n","    def other_expr(df):\n","        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n","        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n","        return  expr_max +expr_last\n","    \n","    def count_expr(df):\n","        cols = [col for col in df.columns if \"num_group\" in col]\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols] \n","        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n","        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n","        return  expr_max +expr_last\n","    \n","    def get_exprs(df):\n","        exprs = Aggregator.num_expr(df) + \\\n","                Aggregator.date_expr(df) + \\\n","                Aggregator.str_expr(df) + \\\n","                Aggregator.other_expr(df) + \\\n","                Aggregator.count_expr(df)\n","\n","        return exprs"]},{"cell_type":"markdown","metadata":{},"source":["## how to read file"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T06:48:27.420248Z","iopub.status.busy":"2024-04-20T06:48:27.419374Z","iopub.status.idle":"2024-04-20T06:48:27.427605Z","shell.execute_reply":"2024-04-20T06:48:27.426506Z","shell.execute_reply.started":"2024-04-20T06:48:27.420217Z"},"trusted":true},"outputs":[],"source":["def read_file(path, depth = None):\n","    df = pl.read_parquet(path)\n","    df = df.pipe(Pipeline.set_table_dtypes)\n","    if depth in [1,2]:\n","        df = df.group_by('case_id').agg(Aggregator.get_exprs(df))\n","    return df\n","\n","def read_files(regex_path,depth = None):\n","    chunks = []\n","    \n","    for path in glob(str(regex_path)):\n","        df = pl.read_parquet(path)\n","        df = df.pipe(Pipeline.set_table_dtypes)\n","        if depth in [1, 2]:\n","            df = df.group_by('case_id').agg(Aggregator.get_exprs(df))\n","        chunks.append(df)\n","        \n","    df = pl.concat(chunks,how = 'vertical_relaxed')\n","    df = df.unique(subset = ['case_id'])\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["## feature engineering: add new col in df_base & left join different depth file togather"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T06:48:29.794946Z","iopub.status.busy":"2024-04-20T06:48:29.794275Z","iopub.status.idle":"2024-04-20T06:48:29.801010Z","shell.execute_reply":"2024-04-20T06:48:29.800027Z","shell.execute_reply.started":"2024-04-20T06:48:29.794916Z"},"trusted":true},"outputs":[],"source":["def feature_eng(df_base, depth_0, depth_1, depth_2):\n","    df_base = (\n","        df_base\n","        .with_columns(\n","            month_decision = pl.col(\"date_decision\").dt.month(),\n","            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n","        )\n","    )\n","    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n","        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n","    df_base = df_base.pipe(Pipeline.handle_dates)\n","    return df_base"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T06:48:33.437335Z","iopub.status.busy":"2024-04-20T06:48:33.436550Z","iopub.status.idle":"2024-04-20T06:48:33.442232Z","shell.execute_reply":"2024-04-20T06:48:33.441253Z","shell.execute_reply.started":"2024-04-20T06:48:33.437307Z"},"trusted":true},"outputs":[],"source":["def to_pandas(dataframe, categorical_columns=None):\n","    # Convert the dataframe to a pandas dataframe\n","    pandas_df = dataframe.to_pandas()\n","    \n","    # If categorical_columns is not provided, identify all object dtype columns as categorical\n","    if categorical_columns is None:\n","        categorical_columns = list(pandas_df.select_dtypes(\"object\").columns)\n","    \n","    # Convert identified columns to categorical dtype\n","    pandas_df[categorical_columns] = pandas_df[categorical_columns].astype(\"category\")\n","    \n","    # Return the pandas dataframe and the list of categorical columns\n","    return pandas_df, categorical_columns"]},{"cell_type":"markdown","metadata":{},"source":["## decrease the memory by using lightest number type"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T06:48:36.246011Z","iopub.status.busy":"2024-04-20T06:48:36.245666Z","iopub.status.idle":"2024-04-20T06:48:36.259715Z","shell.execute_reply":"2024-04-20T06:48:36.258737Z","shell.execute_reply.started":"2024-04-20T06:48:36.245982Z"},"trusted":true},"outputs":[],"source":["def reduce_mem_usage(dataframe):\n","    # Calculate the initial memory usage of the DataFrame\n","    initial_memory_usage = dataframe.memory_usage().sum() / 1024**2\n","    print(f'Initial memory usage: {initial_memory_usage:.2f} MB')\n","    \n","    # Iterate over each column in the DataFrame\n","    for column_name in dataframe.columns:\n","        column_dtype = dataframe[column_name].dtype\n","\n","        # Skip the optimization for categorical columns\n","        if str(column_dtype) == \"category\":\n","            continue\n","\n","        # Skip optimization for non-numeric columns\n","        if column_dtype != 'object':\n","            min_value = dataframe[column_name].min()\n","            max_value = dataframe[column_name].max()\n","\n","            # Downcast integer columns to the smallest integer dtype possible\n","            if 'int' in str(column_dtype):\n","                if min_value >= np.iinfo(np.int8).min and max_value <= np.iinfo(np.int8).max:\n","                    dataframe[column_name] = dataframe[column_name].astype(np.int8)\n","                elif min_value >= np.iinfo(np.int16).min and max_value <= np.iinfo(np.int16).max:\n","                    dataframe[column_name] = dataframe[column_name].astype(np.int16)\n","                elif min_value >= np.iinfo(np.int32).min and max_value <= np.iinfo(np.int32).max:\n","                    dataframe[column_name] = dataframe[column_name].astype(np.int32)\n","                elif min_value >= np.iinfo(np.int64).min and max_value <= np.iinfo(np.int64).max:\n","                    dataframe[column_name] = dataframe[column_name].astype(np.int64)  \n","\n","            # Downcast float columns to the smallest float dtype possible\n","            else:\n","                if min_value >= np.finfo(np.float16).min and max_value <= np.finfo(np.float16).max:\n","                    dataframe[column_name] = dataframe[column_name].astype(np.float16)\n","                elif min_value >= np.finfo(np.float32).min and max_value <= np.finfo(np.float32).max:\n","                    dataframe[column_name] = dataframe[column_name].astype(np.float32)\n","                else:\n","                    dataframe[column_name] = dataframe[column_name].astype(np.float64)\n","\n","        else:\n","            continue\n","\n","    # Calculate the final memory usage after optimization\n","    final_memory_usage = dataframe.memory_usage().sum() / 1024**2\n","    print(f'Final memory usage: {final_memory_usage:.2f} MB (reduced by {(initial_memory_usage - final_memory_usage) / initial_memory_usage * 100:.1f}%)')\n","    \n","    # Return the optimized DataFrame\n","    return dataframe"]},{"cell_type":"markdown","metadata":{},"source":["# load the train files"]},{"cell_type":"markdown","metadata":{},"source":["## define the path"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T06:48:39.389579Z","iopub.status.busy":"2024-04-20T06:48:39.389176Z","iopub.status.idle":"2024-04-20T06:48:39.394382Z","shell.execute_reply":"2024-04-20T06:48:39.393266Z","shell.execute_reply.started":"2024-04-20T06:48:39.389530Z"},"trusted":true},"outputs":[],"source":["ROOT            = Path('/kaggle/input/home-credit-credit-risk-model-stability')\n","TRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\n","TEST_DIR        = ROOT / \"parquet_files\" / \"test\""]},{"cell_type":"markdown","metadata":{},"source":["## load in data_store"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T06:48:42.395526Z","iopub.status.busy":"2024-04-20T06:48:42.395137Z","iopub.status.idle":"2024-04-20T06:51:00.260604Z","shell.execute_reply":"2024-04-20T06:51:00.259709Z","shell.execute_reply.started":"2024-04-20T06:48:42.395497Z"},"trusted":true},"outputs":[],"source":["data_store = {\n","    'df_base': read_file(TRAIN_DIR / 'train_base.parquet'),\n","    'depth_0':[\n","        read_file(TRAIN_DIR / 'train_static_cb_0.parquet'),\n","        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n","    ],\n","     'depth_1': [\n","        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n","        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),  \n","     ],\n","     'depth_2':[\n","        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n","        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n","        read_file(TRAIN_DIR / \"train_applprev_2.parquet\", 2),\n","        read_file(TRAIN_DIR / \"train_person_2.parquet\", 2)\n","         \n","     ]\n","}"]},{"cell_type":"markdown","metadata":{},"source":["## exec feature engi => get the one overall df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T06:51:10.606041Z","iopub.status.busy":"2024-04-20T06:51:10.605439Z","iopub.status.idle":"2024-04-20T06:51:27.407864Z","shell.execute_reply":"2024-04-20T06:51:27.406681Z","shell.execute_reply.started":"2024-04-20T06:51:10.606001Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["train data shape:\t (1526659, 861)\n"]}],"source":["df_train = feature_eng(**data_store)#**传参，不需要提前知道字典中有哪些键，可以更加灵活地传递参数\n","print(\"train data shape:\\t\", df_train.shape)#获得DataFrame维度（行与列）"]},{"cell_type":"markdown","metadata":{},"source":["## Deleted large objects and free up memory sooner"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T07:17:10.427979Z","iopub.status.busy":"2024-04-20T07:17:10.427275Z","iopub.status.idle":"2024-04-20T07:17:10.457673Z","shell.execute_reply":"2024-04-20T07:17:10.456392Z","shell.execute_reply.started":"2024-04-20T07:17:10.427945Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'data_store' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m data_store\n\u001b[1;32m      2\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n","\u001b[0;31mNameError\u001b[0m: name 'data_store' is not defined"]}],"source":["del data_store\n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["## Do the filter for all cols, convert to pandas df & reduce the mem usage of df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_train = df_train.pipe(Pipeline.filter_cols)\n","df_train, cat_cols = to_pandas(df_train)\n","df_train = reduce_mem_usage(df_train)\n","print(\"train data shape:\\t\", df_train.shape)#优化后"]},{"cell_type":"markdown","metadata":{},"source":["# Do some Exploratory Data Analysis (EDA) work"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Understanding the data\n","print(f'The shape of train data{df_train.shape}')  # prints the number of rows and columns\n","print(f'The shape of test data{df_test.shape}')  # prints the number of rows and columns\n","# plot the ratio of train and test data using pie chart\n","plt.figure(figsize=(10, 6))\n","plt.pie([df_train.shape[0], df_test.shape[0]], labels=['Train', 'Test'], autopct='%1.1f%%', startangle=140)\n","plt.title('Train and Test Data Ratio')\n","plt.show()\n","\n","\n","\n","print('-------------------------------------------------')\n","print(f'The columns of train data{df_train.columns}')  # prints the column names\n","print(f'The data types of train data{df_train.dtypes}')  # prints the data types of the columns\n","\n","\n","print('-------------------------------------------------')\n","print('The distribution of data types in train data')\n","data_type_counts = df_train.dtypes.value_counts()  # count the number of columns of each data type\n","\n","# Create a pie chart\n","plt.figure(figsize=(10, 6))\n","plt.pie(data_type_counts, labels=data_type_counts.index, autopct='%1.1f%%', startangle=140)\n","plt.title('Distribution of Data Types in DataFrame')\n","plt.show()\n","\n","print('-------------------------------------------------')\n","print(f'The head infomation of train data{df_train.head()}')  # prints the first few rows\n","print('-------------------------------------------------')\n","\n","\n","# Descriptive statistics\n","print('The summary statistics for train data')\n","print(df_train.describe())  # summary statistics for numerical columns\n","\n","# Checking for missing values\n","print('The number of missing values of train data in each column')\n","print(df_train.isnull().sum())  # prints the number of missing values in each column\n","\n","# Data visualization\n","# Histograms for each variable\n","df_train.hist(figsize=(10,10))\n","plt.show()\n","\n","# Boxplot for each variable\n","df_train.plot(kind='box', subplots=True, layout=(5,5), sharex=False, sharey=False, figsize=(10,10))\n","plt.show()\n","\n","# Correlation matrix\n","corr = df_train.corr()\n","sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Get the group numerical Columns by Missing Value Counts "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nums=df_train.select_dtypes(exclude='category').columns#排除分类变量，选择数值型\n","from itertools import combinations, permutations\n","#df_train=df_train[nums]\n","nans_df = df_train[nums].isna()#判断缺失值\n","nans_groups={}\n","for col in nums:\n","    cur_group = nans_df[col].sum()#缺失值相加\n","    try:\n","        nans_groups[cur_group].append(col)\n","    except:\n","        nans_groups[cur_group]=[col]\n","del nans_df; x=gc.collect()\n","\n","# as the result, get the nans_groups\n","# {\n","#     0: [\"column1\", \"column2\"],  # No missing values in these columns\n","#     5: [\"column3\"],             # 5 missing values in column3\n","#     10: [\"column4\", \"column5\"], # 10 missing values in each of these columns\n","# }"]},{"cell_type":"markdown","metadata":{},"source":["## Define two helper func to handle nans_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# selecting columns based on the maximum number of unique values.\n","def reduce_group(groups):\n","    selected_columns = []  # This will store the column names selected from each group\n","    for group in groups:\n","        max_uniques = 0  # Initialize the maximum number of unique values found\n","        selected_column = group[0]  # Default to the first column in the group as a fallback\n","        \n","        for column in group:\n","            unique_count = df_train[column].nunique()  # Number of unique values in the column\n","            if unique_count > max_uniques:\n","                max_uniques = unique_count\n","                selected_column = column\n","        \n","        selected_columns.append(selected_column)\n","    \n","    print('Selected columns based on max uniques:', selected_columns)\n","    return selected_columns\n","\n","\n","\n","def group_columns_by_correlation(dataframe, correlation_threshold=0.8):\n","    # Calculate the correlation matrix for the dataframe\n","    correlation_matrix = dataframe.corr()\n","\n","    # Initialize a list to hold all groups of correlated columns\n","    correlated_groups = []\n","\n","    # List of columns yet to be grouped\n","    remaining_columns = list(dataframe.columns)\n","\n","    while remaining_columns:\n","        # Pop the first column name off the remaining list to create a new correlation group\n","        current_column = remaining_columns.pop(0)\n","        current_group = [current_column]\n","        correlated_columns = [current_column]  # This will hold all columns that correlate with `current_column`\n","\n","        # Iterate over the remaining columns and check if the correlation with the current column is above the threshold\n","        for other_column in remaining_columns:\n","            if correlation_matrix.loc[current_column, other_column] >= correlation_threshold:\n","                current_group.append(other_column)\n","                correlated_columns.append(other_column)\n","\n","        # Append the group to the list of groups\n","        correlated_groups.append(current_group)\n","\n","        # Update remaining_columns by removing the correlated columns\n","        remaining_columns = [col for col in remaining_columns if col not in correlated_columns]\n","\n","    return correlated_groups\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-19T00:42:16.928436Z","iopub.status.idle":"2024-04-19T00:42:16.928826Z","shell.execute_reply":"2024-04-19T00:42:16.928664Z","shell.execute_reply.started":"2024-04-19T00:42:16.928648Z"},"trusted":true},"outputs":[],"source":["df_train = feature_eng(**data_store)#**传参，不需要提前知道字典中有哪些键，可以更加灵活地传递参数\n","print(\"train data shape:\\t\", df_train.shape)#获得DataFrame维度（行与列）\n","del data_store\n","gc.collect()#内存回收\n","df_train = df_train.pipe(Pipeline.filter_cols)\n","df_train, cat_cols = to_pandas(df_train)\n","df_train = reduce_mem_usage(df_train)\n","print(\"train data shape:\\t\", df_train.shape)#优化后\n","\n","nums=df_train.select_dtypes(exclude='category').columns#排除分类变量，选择数值型\n","from itertools import combinations, permutations\n","#df_train=df_train[nums]\n","nans_df = df_train[nums].isna()#判断缺失值\n","nans_groups={}\n","for col in nums:\n","    cur_group = nans_df[col].sum()#缺失值相加\n","    try:\n","        nans_groups[cur_group].append(col)\n","    except:\n","        nans_groups[cur_group]=[col]\n","del nans_df; x=gc.collect()\n","\n","# selecting columns based on the maximum number of unique values.\n","def reduce_group(groups):\n","    selected_columns = []  # This will store the column names selected from each group\n","    for group in groups:\n","        max_uniques = 0  # Initialize the maximum number of unique values found\n","        selected_column = group[0]  # Default to the first column in the group as a fallback\n","        \n","        for column in group:\n","            unique_count = df_train[column].nunique()  # Number of unique values in the column\n","            if unique_count > max_uniques:\n","                max_uniques = unique_count\n","                selected_column = column\n","        \n","        selected_columns.append(selected_column)\n","    \n","    print('Selected columns based on max uniques:', selected_columns)\n","    return selected_columns\n","\n","\n","\n","def group_columns_by_correlation(dataframe, correlation_threshold=0.8):\n","    # Calculate the correlation matrix for the dataframe\n","    correlation_matrix = dataframe.corr()\n","\n","    # Initialize a list to hold all groups of correlated columns\n","    correlated_groups = []\n","\n","    # List of columns yet to be grouped\n","    remaining_columns = list(dataframe.columns)\n","\n","    while remaining_columns:\n","        # Pop the first column name off the remaining list to create a new correlation group\n","        current_column = remaining_columns.pop(0)\n","        current_group = [current_column]\n","        correlated_columns = [current_column]  # This will hold all columns that correlate with `current_column`\n","\n","        # Iterate over the remaining columns and check if the correlation with the current column is above the threshold\n","        for other_column in remaining_columns:\n","            if correlation_matrix.loc[current_column, other_column] >= correlation_threshold:\n","                current_group.append(other_column)\n","                correlated_columns.append(other_column)\n","\n","        # Append the group to the list of groups\n","        correlated_groups.append(current_group)\n","\n","        # Update remaining_columns by removing the correlated columns\n","        remaining_columns = [col for col in remaining_columns if col not in correlated_columns]\n","\n","    return correlated_groups\n","\n","uses=[]\n","for k,v in nans_groups.items():\n","    if len(v)>1:\n","            Vs = nans_groups[k]\n","            #cross_features=list(combinations(Vs, 2))\n","            #make_corr(Vs)\n","            grps= group_columns_by_correlation(df_train[Vs], threshold=0.8)\n","            use=reduce_group(grps)\n","            uses=uses+use\n","            #make_corr(use)\n","    else:\n","        uses=uses+v\n","    print('####### NAN count =',k)\n","print(uses)\n","print(len(uses))\n","uses=uses+list(df_train.select_dtypes(include='category').columns)\n","print(len(uses))\n","df_train=df_train[uses]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-19T00:42:16.930122Z","iopub.status.idle":"2024-04-19T00:42:16.930534Z","shell.execute_reply":"2024-04-19T00:42:16.930363Z","shell.execute_reply.started":"2024-04-19T00:42:16.930346Z"},"trusted":true},"outputs":[],"source":["sample = pd.read_csv(\"/kaggle/input/home-credit-credit-risk-model-stability/sample_submission.csv\")\n","device='gpu'\n","#n_samples=200000\n","n_est=6000\n","DRY_RUN = True if sample.shape[0] == 10 else False   \n","if DRY_RUN:\n","    device='cpu'\n","    df_train = df_train.iloc[:50000]\n","    #n_samples=10000\n","    n_est = 600\n","print(device)"]},{"cell_type":"markdown","metadata":{},"source":["# load the test file into data_store"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-19T00:42:16.931854Z","iopub.status.idle":"2024-04-19T00:42:16.932180Z","shell.execute_reply":"2024-04-19T00:42:16.932030Z","shell.execute_reply.started":"2024-04-19T00:42:16.932016Z"},"trusted":true},"outputs":[],"source":["data_store = {\n","    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n","    \"depth_0\": [\n","        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n","        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n","    ],\n","    \"depth_1\": [\n","        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n","        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n","        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n","        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n","        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n","        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n","        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n","        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n","        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n","        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n","    ],\n","    \"depth_2\": [\n","        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n","        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n","        read_file(TEST_DIR / \"test_applprev_2.parquet\", 2),\n","        read_file(TEST_DIR / \"test_person_2.parquet\", 2)\n","    ]\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-19T00:42:16.936066Z","iopub.status.idle":"2024-04-19T00:42:16.936449Z","shell.execute_reply":"2024-04-19T00:42:16.936287Z","shell.execute_reply.started":"2024-04-19T00:42:16.936271Z"},"trusted":true},"outputs":[],"source":["df_test = feature_eng(**data_store)\n","print(\"test data shape:\\t\", df_test.shape)\n","del data_store\n","gc.collect()\n","df_test = df_test.select([col for col in df_train.columns if col != \"target\"])\n","print(\"train data shape:\\t\", df_train.shape)\n","print(\"test data shape:\\t\", df_test.shape)\n","\n","df_test, cat_cols = to_pandas(df_test, cat_cols)\n","df_test = reduce_mem_usage(df_test)\n","\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-19T00:42:16.937587Z","iopub.status.idle":"2024-04-19T00:42:16.937936Z","shell.execute_reply":"2024-04-19T00:42:16.937777Z","shell.execute_reply.started":"2024-04-19T00:42:16.937762Z"},"trusted":true},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","#遍历键k-值v\n","for k, v in nans_groups.items():\n","    if len(v) > 1:\n","        # 检查列是否存在于DataFrame中\n","        if all(col in df_train.columns for col in v):\n","            # 计算相关性矩阵\n","            correlation_matrix = df_train[v].corr()\n","            \n","            # 绘制热力图\n","            plt.figure(figsize=(10, 8))\n","            sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n","            plt.title(f'Correlation Matrix (NAN count = {k})')\n","            plt.show()\n","        else:\n","            print(f\"Columns {v} not found in DataFrame.\")\n","\n","    print('####### NAN count =', k)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-19T00:42:16.940385Z","iopub.status.idle":"2024-04-19T00:42:16.941128Z","shell.execute_reply":"2024-04-19T00:42:16.940886Z","shell.execute_reply.started":"2024-04-19T00:42:16.940864Z"},"trusted":true},"outputs":[],"source":["y = df_train ['target']\n","weeks = df_train['WEEK_NUM']\n","df_train = df_train.drop(columns = ['target','case_id','WEEK_NUM'])\n","cv = StratifiedGroupKFold(n_splits=5, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-19T00:42:16.943055Z","iopub.status.idle":"2024-04-19T00:42:16.943449Z","shell.execute_reply":"2024-04-19T00:42:16.943276Z","shell.execute_reply.started":"2024-04-19T00:42:16.943259Z"},"trusted":true},"outputs":[],"source":["df_train[cat_cols] = df_train[cat_cols].astype(str)\n","df_test[cat_cols] = df_test[cat_cols].astype(str)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-19T00:42:16.944956Z","iopub.status.idle":"2024-04-19T00:42:16.945385Z","shell.execute_reply":"2024-04-19T00:42:16.945181Z","shell.execute_reply.started":"2024-04-19T00:42:16.945165Z"},"trusted":true},"outputs":[],"source":["from catboost import CatBoostClassifier, Pool\n","\n","fitted_models = []\n","cv_scores = []\n","\n","\n","for idx_train, idx_valid in cv.split(df_train, y, groups=weeks):#\n","    X_train, y_train = df_train.iloc[idx_train], y.iloc[idx_train]# \n","    X_valid, y_valid = df_train.iloc[idx_valid], y.iloc[idx_valid]\n","    train_pool = Pool(X_train, y_train,cat_features=cat_cols)\n","    val_pool = Pool(X_valid, y_valid,cat_features=cat_cols)\n","    clf = CatBoostClassifier(\n","    eval_metric='AUC',\n","    task_type='GPU',\n","    learning_rate=0.03,\n","    iterations=n_est)\n","    clf.fit(train_pool, eval_set=val_pool,verbose=300)\n","    fitted_models.append(clf)\n","    y_pred_valid = clf.predict_proba(X_valid)[:,1]\n","    auc_score = roc_auc_score(y_valid, y_pred_valid)\n","    cv_scores.append(auc_score)\n","    \n","print(\"CV AUC scores: \", cv_scores)\n","print(\"Maximum CV AUC score: \", max(cv_scores))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-19T00:42:16.946859Z","iopub.status.idle":"2024-04-19T00:42:16.947272Z","shell.execute_reply":"2024-04-19T00:42:16.947060Z","shell.execute_reply.started":"2024-04-19T00:42:16.947044Z"},"trusted":true},"outputs":[],"source":["class VotingModel(BaseEstimator, RegressorMixin):\n","    def __init__(self, estimators):\n","        super().__init__()\n","        self.estimators = estimators\n","        \n","    def fit(self, X, y=None):\n","        return self\n","    \n","    def predict(self, X):\n","        y_preds = [estimator.predict(X) for estimator in self.estimators]\n","        return np.mean(y_preds, axis=0)\n","    \n","    def predict_proba(self, X):\n","        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n","        return np.mean(y_preds, axis=0)\n","\n","model = VotingModel(fitted_models)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-19T00:42:16.949173Z","iopub.status.idle":"2024-04-19T00:42:16.949708Z","shell.execute_reply":"2024-04-19T00:42:16.949481Z","shell.execute_reply.started":"2024-04-19T00:42:16.949458Z"},"trusted":true},"outputs":[],"source":["feature_importance =fitted_models[2].get_feature_importance(type='PredictionValuesChange')\n","\n","# 获取特征名称\n","feature_names = X_train.columns\n","\n","# 对特征重要性进行排序\n","sorted_idx = np.argsort(feature_importance)\n","\n","# 绘制特征重要性图\n","plt.figure(figsize=(20, 60))\n","plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\n","plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n","plt.xlabel('Feature Importance')\n","plt.ylabel('Features')\n","plt.title('CatBoost Feature Importance')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-19T00:42:16.951875Z","iopub.status.idle":"2024-04-19T00:42:16.952375Z","shell.execute_reply":"2024-04-19T00:42:16.952123Z","shell.execute_reply.started":"2024-04-19T00:42:16.952102Z"},"trusted":true},"outputs":[],"source":["df_test = df_test.drop(columns=[\"WEEK_NUM\"])\n","df_test = df_test.set_index(\"case_id\")\n","\n","\n","y_pred = pd.Series(model.predict_proba(df_test)[:, 1], index=df_test.index)\n","df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n","df_subm = df_subm.set_index(\"case_id\")\n","\n","df_subm[\"score\"] = y_pred\n","df_subm.to_csv(\"submission.csv\")\n","df_subm"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7921029,"sourceId":50160,"sourceType":"competition"}],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}
